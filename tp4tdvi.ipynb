{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1iLx2DRitxx"
      },
      "source": [
        "Universidad Torcuato Di Tella\n",
        "\n",
        "Licenciatura en Tecnología Digital\\\n",
        "**Tecnología Digital VI: Inteligencia Artificial**\n",
        "\n",
        "Integrantes: Isabel Núñez, Camilo Suárez y Valentina Vitetta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSdBL2673KUX"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as tt\n",
        "from google.colab import drive\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAR3tiGci2-e"
      },
      "source": [
        "\n",
        "# TP4: Encodeador de música"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1"
      ],
      "metadata": {
        "id": "2lInXTmmKAwz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5G8mTE-5zM"
      },
      "source": [
        "## Conectamos la notebook a gdrive y seteamos data_dir con el path a los archivos.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTLQpzk2e85f"
      },
      "source": [
        "Modificar data_dir con el path adecuado que lleve a la carpeta genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5AUydgIxfwi"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "data_dir='//content/drive/MyDrive/tp3tdvi/genres_5sec/'\n",
        "list_files=os.listdir(data_dir)\n",
        "classes=[]\n",
        "for file in list_files:\n",
        "  name='{}/{}'.format(data_dir,file)\n",
        "  if os.path.isdir(name):\n",
        "    classes.append(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFH8nbgxfUTZ"
      },
      "source": [
        "## Creamos una clase para manejar los audios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJxZV04XZtnP"
      },
      "outputs": [],
      "source": [
        "samplerate=22050\n",
        "def parse_genres(fname):\n",
        "    parts = fname.split('/')[-1].split('.')[0]\n",
        "    return parts\n",
        "\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.files =[]\n",
        "        for c in classes:\n",
        "          self.files = self.files + [fname for fname in os.listdir(os.path.join(root,c)) if fname.endswith('.wav')]\n",
        "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        fname = self.files[i]\n",
        "        genre = parse_genres(fname)\n",
        "        fpath = os.path.join(self.root,genre, fname)\n",
        "        class_idx = self.classes.index(genre)\n",
        "        audio = torchaudio.load(fpath)[0]\n",
        "\n",
        "        return audio, class_idx\n",
        "\n",
        "dataset = MusicDataset(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKXtKn7kgId7"
      },
      "source": [
        "## Dividimos el conjunto de datos en entrenamiento, validación y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dr5Qhgk5sjL"
      },
      "outputs": [],
      "source": [
        "random_seed = 42 # Semilla para reproducibilidad\n",
        "torch.manual_seed(random_seed)\n",
        "val_size = 100\n",
        "test_size = 100\n",
        "train_size = len(dataset) - val_size - test_size\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "len(train_ds),len(val_ds),len(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sDPIuQAga7B"
      },
      "source": [
        "## Creamos los DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBHjbBoo5sG1"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "num_workers = 2\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "valid_dl = DataLoader(val_ds, batch_size*2, num_workers=num_workers, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, 1, shuffle=True, num_workers=num_workers, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXnHDpoYttX8"
      },
      "source": [
        "## Creamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pO-p4Tbm4Ll"
      },
      "outputs": [],
      "source": [
        "# Para el ejercicio 2 de análisis, cambiar la cantidad de FM en el vector latente\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc_conv1 = nn.Conv1d(1, 64, kernel_size=1024, padding=512, stride=512) # Espacio latente = 64 x 216\n",
        "\n",
        "        # Decoder\n",
        "        self.dec_conv1 = nn.ConvTranspose1d(64, 1, kernel_size=1194, padding=512, stride=512)\n",
        "\n",
        "    def forward_encoder(self, x):\n",
        "        x = self.enc_conv1(x)\n",
        "        return x\n",
        "\n",
        "    def forward_decoder(self, x):\n",
        "        x = torch.tanh(self.dec_conv1(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_encoder(x)\n",
        "        x = self.forward_decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6l6uFsiiUey"
      },
      "source": [
        "## Configuramos el dispositivo en el que se entrenará el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryss1Hhm3KUf"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "autoencoder = Autoencoder().to(device)\n",
        "print(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBhUoGMgiryi"
      },
      "source": [
        "## Seteamos algunos hiperparámetros y comenzamos a entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6cJrYPk8V8J"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0005\n",
        "num_epochs = 30\n",
        "loss_function = nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_losses = []\n",
        "\n",
        "    for wav, genre_index in train_dl:\n",
        "        wav = wav.to(device)\n",
        "\n",
        "        # Forward\n",
        "        out = autoencoder(wav)\n",
        "\n",
        "        loss = loss_function(out, wav)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        del wav #importante para ir liberando memoria ram\n",
        "        del genre_index #importante para ir liberando memoria ram\n",
        "        del loss #importante para ir liberando memoria ram\n",
        "        del out  #importante para ir liberando memoria ram\n",
        "        torch.cuda.empty_cache()  #importante para ir liberando memoria ram\n",
        "        gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "\n",
        "    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, train_loss))\n",
        "\n",
        "    # Validation\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for wav, genre_index in valid_dl:\n",
        "            wav = wav.to(device)\n",
        "\n",
        "            out = autoencoder(wav)\n",
        "\n",
        "            loss = loss_function(out, wav)\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "            del wav #importante para ir liberando memoria ram\n",
        "            del genre_index #importante para ir liberando memoria ram\n",
        "            del loss #importante para ir liberando memoria ram\n",
        "            del out  #importante para ir liberando memoria ram\n",
        "            torch.cuda.empty_cache()  #importante para ir liberando memoria ram\n",
        "            gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "    valid_loss = np.mean(val_losses)\n",
        "    print('Epoch: [%d/%d], Valid loss: %.4f' % (epoch+1, num_epochs, valid_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardamos el modelo"
      ],
      "metadata": {
        "id": "LRwEw6ABH9Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(autoencoder.state_dict(), '64FM.ckpt')"
      ],
      "metadata": {
        "id": "3uoxrLpbIfqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con el modelo entrenado, probamos reconstruyendo alguna canción"
      ],
      "metadata": {
        "id": "rVUpGOr0HHyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTeeky4wZA3X"
      },
      "outputs": [],
      "source": [
        "# Cargar algun ejemplo del dataset\n",
        "waveform, label = dataset[22]\n",
        "\n",
        "# Mover el waveform al mismo dispositivo que el modelo (GPU o CPU)\n",
        "waveform = waveform.to(device)\n",
        "\n",
        "# Reproducir el audio original\n",
        "IPython.display.Audio(waveform.cpu().numpy(), rate=samplerate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtawM6P3aPm8"
      },
      "outputs": [],
      "source": [
        "# Ahora con\n",
        "with torch.no_grad():\n",
        "    outputs = autoencoder(waveform)  # La salida también estará en el mismo dispositivo\n",
        "    latent_space = autoencoder.forward_encoder(waveform)\n",
        "\n",
        "print(f'tamaño espacio latente: {latent_space.shape}')\n",
        "# Mover las salidas a la CPU si están en la GPU para reproducción\n",
        "outputs = outputs.cpu().numpy()\n",
        "\n",
        "# Reproducir el audio descomprimido\n",
        "IPython.display.Audio(outputs, rate=samplerate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFwYdlWxCN0M"
      },
      "source": [
        "## Evaluamos el modelo con el conjunto de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pqtx-D0zAwa"
      },
      "outputs": [],
      "source": [
        "# Cargamos el modelo si es necesario\n",
        "S = torch.load('64FM.ckpt')\n",
        "autoencoder.load_state_dict(S)\n",
        "\n",
        "# Test\n",
        "autoencoder.eval()\n",
        "test_losses = []\n",
        "with torch.no_grad():\n",
        "    for wav, genre_index in test_dl:\n",
        "        wav = wav.to(device)\n",
        "\n",
        "        out = autoencoder(wav)\n",
        "\n",
        "        loss = loss_function(out, wav)\n",
        "\n",
        "        test_losses.append(loss.item())\n",
        "\n",
        "        del wav #importante para ir liberando memoria ram\n",
        "        del genre_index #importante para ir liberando memoria ram\n",
        "        del loss #importante para ir liberando memoria ram\n",
        "        del out  #importante para ir liberando memoria ram\n",
        "        torch.cuda.empty_cache()  #importante para ir liberando memoria ram\n",
        "        gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "test_loss = np.mean(test_losses)\n",
        "print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2"
      ],
      "metadata": {
        "id": "jqkqiur3KhIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargamos los pesos de nuestro modelo entrenado"
      ],
      "metadata": {
        "id": "szUwHhoBVxcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.load_state_dict(torch.load('64FM.ckpt'))"
      ],
      "metadata": {
        "id": "BVfe-xfrKlWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(train_ds)  # Número de canciones\n",
        "latent_vector_dim = 64 * 216  # Dimensión del vector latente aplanado\n",
        "\n",
        "latent_vectors = np.zeros((num_samples, latent_vector_dim))"
      ],
      "metadata": {
        "id": "Kl3liTSwV4Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generamos los vectores latentes"
      ],
      "metadata": {
        "id": "ZQtoP98aWlNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "with torch.no_grad():\n",
        "    for wav, genre_index in train_ds:\n",
        "        wav = wav.to(device)\n",
        "\n",
        "        # Forward_encoder\n",
        "        latent_vector = autoencoder.forward_encoder(wav)\n",
        "\n",
        "        latent_vectors[i, :] = latent_vector.flatten().cpu().numpy()\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        del wav #importante para ir liberando memoria ram\n",
        "        del genre_index #importante para ir liberando memoria ram\n",
        "        del latent_vector  #importante para ir liberando memoria ram\n",
        "        torch.cuda.empty_cache()  #importante para ir liberando memoria ram\n",
        "        gc.collect() #importante para ir liberando memoria ram"
      ],
      "metadata": {
        "id": "0nI9gxjVWibk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método del codo"
      ],
      "metadata": {
        "id": "_QOFcUWCWtMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inertia = []\n",
        "k_values = range(1, 14)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(latent_vectors)\n",
        "    inertia.append(kmeans.inertia_)"
      ],
      "metadata": {
        "id": "2_WCCgfdWzw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, inertia, marker='o', linestyle='--')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia (SSE)')\n",
        "plt.title('Método del Codo')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A7XdNYJCW7S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "9dVcuaBvXBjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 8  # Número de clusters\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(latent_vectors)\n",
        "\n",
        "#Cantidad de puntos en cada cluster\n",
        "unique, counts = np.unique(clusters, return_counts=True)\n",
        "for cluster_id, count in zip(unique, counts):\n",
        "    print(f\"Cluster {cluster_id}: {count} puntos\")\n"
      ],
      "metadata": {
        "id": "qx0UMY9PXDyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "3N8qOtsKXeL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_latent_vectors = latent_vectors.reshape(latent_vectors.shape[0], -1)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_latent_vectors = pca.fit_transform(flattened_latent_vectors)"
      ],
      "metadata": {
        "id": "GsD-3t6yYaED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar los datos de PCA\n",
        "plt.scatter(reduced_latent_vectors[:, 0], reduced_latent_vectors[:, 1], cmap='viridis')\n",
        "plt.title(f'PCA')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PYkN4HArXg77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buscamos los outliers del gráfico y los reproducimos"
      ],
      "metadata": {
        "id": "yuy2Z1Y3YoSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EJ: Encontrar el índice del valor máximo en el primer componente (componente 0 en PCA)\n",
        "max_index = np.argmax(reduced_latent_vectors[:, 0])\n",
        "\n",
        "print(f\"Índice del vector con el valor máximo en el primer componente: {max_index}\")\n",
        "print(f\"Valor máximo en el primer componente: {reduced_latent_vectors[max_index, 0]}\")"
      ],
      "metadata": {
        "id": "rnAKapslYw2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, genre_index = train_ds[303]  # Usamos el indice obtenido para reproducir la canción\n",
        "IPython.display.display(IPython.display.Audio(waveform.cpu().numpy(), rate=samplerate))"
      ],
      "metadata": {
        "id": "ooy_psiGZAbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 3"
      ],
      "metadata": {
        "id": "GMuiBOitKiuH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VhWiLTPAvDK"
      },
      "source": [
        "## Creamos una clase de dataset personalizado para cargar música nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVBtFC0EBQJO"
      },
      "outputs": [],
      "source": [
        "# Dataset personalizado para cargar archivos WAV\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, audio_dir, transform=None):\n",
        "        self.audio_dir = audio_dir\n",
        "        self.audio_files = [os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Cargar el archivo de audio\n",
        "        audio_path = self.audio_files[idx]\n",
        "        audio = AudioSegment.from_file(audio_path).set_frame_rate(16000).set_channels(1)\n",
        "        audio_data = np.array(audio.get_array_of_samples(), dtype=np.float32) / 32768.0  # Normalizar a [-1, 1]\n",
        "        audio_data = torch.tensor(audio_data).unsqueeze(0)  # Convertir a tensor (1, L)\n",
        "\n",
        "        if self.transform:\n",
        "            audio_data = self.transform(audio_data)\n",
        "\n",
        "        return audio_data, audio_path  # Devolvemos también el path para referencia si es necesario"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargamos audios nuevos"
      ],
      "metadata": {
        "id": "97WoxwHaMv30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbnMUM4IFmq2"
      },
      "outputs": [],
      "source": [
        "# Ruta al dataset\n",
        "dataset_path = '/content/drive/MyDrive/Dataset/audios_wav'\n",
        "\n",
        "# Crear dataset y DataLoader\n",
        "val_dataset = AudioDataset(dataset_path)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconstruimos y comparamos"
      ],
      "metadata": {
        "id": "QIBkr93lNpo2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTBWOtKIF6ZN"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "with torch.no_grad():\n",
        "    for audio, audio_path in val_loader:\n",
        "        audio = audio.to(device)\n",
        "\n",
        "        # Pasar por el autoencoder\n",
        "        reconstructed = autoencoder(audio)\n",
        "\n",
        "        # Convertir de tensores a numpy\n",
        "        original_audio = audio.squeeze().cpu().numpy()\n",
        "        reconstructed_audio = reconstructed.squeeze().cpu().numpy()\n",
        "\n",
        "        print(f\"Reconstruyendo: {audio_path[0]}\")\n",
        "\n",
        "        # Reproducir el audio original\n",
        "        print(\"Audio Original:\")\n",
        "        display(Audio(original_audio, rate=16000))\n",
        "\n",
        "        # Reproducir el audio reconstruido\n",
        "        print(\"Audio Reconstruido:\")\n",
        "        display(Audio(reconstructed_audio, rate=16000))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 4"
      ],
      "metadata": {
        "id": "63n15CngKkLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtenemos los vectores latentes de dos canciones, que serán utilizados para generar música"
      ],
      "metadata": {
        "id": "OMpl98VAP4KS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L29ddOGS_yE"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    audio1, _ = dataset[555]  # Primera canción\n",
        "    audio2, _ = dataset[443]  # Segunda canción\n",
        "\n",
        "    audio1 = audio1.unsqueeze(0).to(device)  # Agregar batch dimension\n",
        "    audio2 = audio2.unsqueeze(0).to(device)\n",
        "\n",
        "    latent1 = autoencoder.forward_encoder(audio1)\n",
        "    latent2 = autoencoder.forward_encoder(audio2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probamos combinando los audios por mitades"
      ],
      "metadata": {
        "id": "duA9C78oP5rG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzHG_qJfYu00"
      },
      "outputs": [],
      "source": [
        "midpoint = latent1.size(2) // 2\n",
        "\n",
        "# Combinar la mitad inicial de la primera canción con la mitad final de la segunda\n",
        "mixed_latent = torch.cat((latent1[:, :, :midpoint], latent2[:, :, midpoint:]), dim=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMV-YjcdYzfh"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    mixed_audio = autoencoder.forward_decoder(mixed_latent)\n",
        "    mixed_audio = mixed_audio.squeeze().cpu().numpy()  # Convertir a numpy para guardar o reproducir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrLOa-reY24v"
      },
      "outputs": [],
      "source": [
        "# Reproducir el audio\n",
        "display(Audio(mixed_audio, rate=16000))\n",
        "\n",
        "# Guardar el audio combinado\n",
        "sf.write(\"mixed_song.wav\", mixed_audio, samplerate=16000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probamos con interpolación"
      ],
      "metadata": {
        "id": "4IAguswyQEao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mowgPBraXa6"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "# Interpolación lineal entre dos vectores latentes\n",
        "alpha_values = torch.linspace(0, 1, steps=5)\n",
        "interpolated_audios = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for alpha in alpha_values:\n",
        "        # Generar interpolación en el espacio latente\n",
        "        interpolated_latent = (1 - alpha) * latent1 + alpha * latent2\n",
        "\n",
        "        # Decodificar el vector interpolado\n",
        "        interpolated_audio = autoencoder.forward_decoder(interpolated_latent)\n",
        "        interpolated_audios.append(interpolated_audio.squeeze().cpu().numpy())\n",
        "\n",
        "# Reproducir cada audio generado\n",
        "for i, audio in enumerate(interpolated_audios):\n",
        "    print(f\"Interpolación {i+1}/{len(interpolated_audios)}:\")\n",
        "    ipd.display(ipd.Audio(audio, rate=samplerate))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probamos alterando una dimensión del vector latente"
      ],
      "metadata": {
        "id": "n_3hgZeBRqEn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHz38VL2bdR1"
      },
      "outputs": [],
      "source": [
        "# Alterar una dimensión específica del vector latente\n",
        "latent_modified = latent1.clone()  # Copiar el vector latente original\n",
        "latent_modified[:, :, 0] += 2.0  # Incrementar la primera dimensión en 2\n",
        "\n",
        "# Reconstrucción\n",
        "with torch.no_grad():\n",
        "    modified_audio = autoencoder.forward_decoder(latent_modified)\n",
        "    modified_audio = modified_audio.squeeze().cpu().numpy()\n",
        "\n",
        "# Reproducir el audio modificado\n",
        "print(\"Audio modificado tras alterar el vector latente:\")\n",
        "ipd.display(ipd.Audio(modified_audio, rate=samplerate))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probamos agregando ruido al vector latente"
      ],
      "metadata": {
        "id": "cwBlgTkhNgHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnliGQ49byYG"
      },
      "outputs": [],
      "source": [
        "# Agregar ruido aleatorio al vector latente\n",
        "noise_factor = 0.1\n",
        "noise = torch.randn_like(latent1) * noise_factor\n",
        "\n",
        "latent_with_noise = latent1 + noise  # Vector latente modificado con ruido\n",
        "\n",
        "# Decodificación\n",
        "with torch.no_grad():\n",
        "    audio_with_noise = autoencoder.forward_decoder(latent_with_noise)\n",
        "    audio_with_noise = audio_with_noise.squeeze().cpu().numpy()\n",
        "\n",
        "# Reproducir el audio con ruido añadido\n",
        "print(\"Audio generado con ruido añadido al vector latente:\")\n",
        "ipd.display(ipd.Audio(audio_with_noise, rate=samplerate))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probamos generando un vector latente aleatorio"
      ],
      "metadata": {
        "id": "h92XCBpOSBcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDc1ZLnviUGW"
      },
      "outputs": [],
      "source": [
        "# Generar un vector latente aleatorio con el mismo tamaño que los vectores latentes originales\n",
        "random_latent = torch.randn_like(latent1).to(device)  # Asegúrate de que esté en el mismo dispositivo que el modelo\n",
        "\n",
        "# Decodificación\n",
        "with torch.no_grad():\n",
        "    random_audio = autoencoder.forward_decoder(random_latent)\n",
        "    random_audio = random_audio.squeeze().cpu().numpy()\n",
        "\n",
        "# Reproducir el audio generado aleatoriamente\n",
        "print(\"Audio generado completamente aleatorio:\")\n",
        "ipd.display(ipd.Audio(random_audio, rate=samplerate))  # Asegúrate de usar el mismo `samplerate`\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7sDPIuQAga7B",
        "j6l6uFsiiUey",
        "szUwHhoBVxcH",
        "ZQtoP98aWlNS",
        "_QOFcUWCWtMR",
        "9dVcuaBvXBjf"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}